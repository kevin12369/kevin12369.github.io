<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.2" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://www.baidu.com/aigc/AI%E6%A1%86%E6%9E%B6/2025%E5%B9%B4%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%A1%86%E6%9E%B6%E5%85%A8%E8%A7%A3%E6%9E%90%EF%BC%9AvLLM%20vs%20Ollama.html"><meta property="og:site_name" content="Kevin的博客"><meta property="og:title" content="2025年大模型部署框架全解析：vLLM vs Ollama"><meta property="og:description" content="2025年大模型部署框架全解析：vLLM vs Ollama 从个人实验到企业生产，选择最适合你的部署方案 引言 2025年，大模型本地部署已成为AI应用落地的关键环节。随着vLLM和Ollama等开源框架的成熟，开发者可以轻松在本地运行各类开源大语言模型。本文将深入对比这两大主流部署框架，帮助你选择最适合的方案。 框架概述 vLLM：高性能推理引擎 定位： 企业级、高性能推理服务 背景："><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2026-01-02T07:50:24.000Z"><meta property="article:author" content="Kevin"><meta property="article:tag" content="vLLM"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="部署"><meta property="article:published_time" content="2025-12-26T00:00:00.000Z"><meta property="article:modified_time" content="2026-01-02T07:50:24.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"2025年大模型部署框架全解析：vLLM vs Ollama","image":[""],"datePublished":"2025-12-26T00:00:00.000Z","dateModified":"2026-01-02T07:50:24.000Z","author":[{"@type":"Person","name":"Kevin","url":"https://www.baidu.com"}]}</script><title>2025年大模型部署框架全解析：vLLM vs Ollama | Kevin的博客</title><meta name="description" content="2025年大模型部署框架全解析：vLLM vs Ollama 从个人实验到企业生产，选择最适合你的部署方案 引言 2025年，大模型本地部署已成为AI应用落地的关键环节。随着vLLM和Ollama等开源框架的成熟，开发者可以轻松在本地运行各类开源大语言模型。本文将深入对比这两大主流部署框架，帮助你选择最适合的方案。 框架概述 vLLM：高性能推理引擎 定位： 企业级、高性能推理服务 背景：">
    <link rel="preload" href="/assets/style-YVseZekw.css" as="style"><link rel="stylesheet" href="/assets/style-YVseZekw.css">
    <link rel="modulepreload" href="/assets/app-D3ipPn3n.js"><link rel="modulepreload" href="/assets/2025年大模型部署框架全解析：vLLM vs Ollama.html-jb5mQ3lH.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js"><link rel="modulepreload" href="/assets/2025年大模型部署框架全解析：vLLM vs Ollama.html-DPosgzEv.js">
    
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><img class="vp-nav-logo" src="/logo.svg" alt="Kevin的博客"><!----><span class="vp-site-name hide-in-pad">Kevin的博客</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="首页" class="vp-link nav-link nav-link" href="/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>首页<!----></a></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="AIGC"><span class="title"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="robot" width="1em" height="1em"></iconify-icon>AIGC</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>大语言模型</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="OpenAI接口文档" class="vp-link nav-link nav-link" href="/aigc/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/OpenAI%E6%8E%A5%E5%8F%A3%E6%96%87%E6%A1%A3.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="code" width="1em" height="1em"></iconify-icon>OpenAI接口文档<!----></a></li><li class="dropdown-subitem"><a aria-label="Llama 3：Meta开源大模型的进化之路" class="vp-link nav-link nav-link" href="/aigc/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/Llama%203%EF%BC%9AMeta%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="language" width="1em" height="1em"></iconify-icon>Llama 3：Meta开源大模型的进化之路<!----></a></li><li class="dropdown-subitem"><a aria-label="2025年大模型技术突破与趋势" class="vp-link nav-link nav-link" href="/aigc/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/2025%E5%B9%B4%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E7%AA%81%E7%A0%B4%E4%B8%8E%E8%B6%8B%E5%8A%BF.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="trending-up" width="1em" height="1em"></iconify-icon>2025年大模型技术突破与趋势<!----></a></li></ul></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>AI框架</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="LangChain：构建AI智能体的操作系统" class="vp-link nav-link nav-link" href="/aigc/AI%E6%A1%86%E6%9E%B6/LangChain%EF%BC%9A%E6%9E%84%E5%BB%BAAI%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="robot" width="1em" height="1em"></iconify-icon>LangChain：构建AI智能体的操作系统<!----></a></li><li class="dropdown-subitem"><a aria-label="2025年大模型部署框架全解析：vLLM vs Ollama" class="vp-link nav-link active nav-link active" href="/aigc/AI%E6%A1%86%E6%9E%B6/2025%E5%B9%B4%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%A1%86%E6%9E%B6%E5%85%A8%E8%A7%A3%E6%9E%90%EF%BC%9AvLLM%20vs%20Ollama.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="server" width="1em" height="1em"></iconify-icon>2025年大模型部署框架全解析：vLLM vs Ollama<!----></a></li></ul></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>本周AIGC资讯</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="第2期" class="vp-link nav-link nav-link" href="/aigc/AI%E5%BA%94%E7%94%A8/%E6%9C%AC%E5%91%A8AIGC%E8%B5%84%E8%AE%AF-%E7%AC%AC2%E6%9C%9F.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fire" width="1em" height="1em"></iconify-icon>第2期<!----></a></li><li class="dropdown-subitem"><a aria-label="第1期" class="vp-link nav-link nav-link" href="/aigc/AI%E5%BA%94%E7%94%A8/%E6%9C%AC%E5%91%A8AIGC%E8%B5%84%E8%AE%AF-%E7%AC%AC1%E6%9C%9F.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fire" width="1em" height="1em"></iconify-icon>第1期<!----></a></li></ul></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>年度回顾</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="2025年AIGC年度回顾" class="vp-link nav-link nav-link" href="/aigc/AI%E5%BA%94%E7%94%A8/2025%E5%B9%B4AIGC%E5%B9%B4%E5%BA%A6%E5%9B%9E%E9%A1%BE%E7%89%B9%E5%88%8A.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="trophy" width="1em" height="1em"></iconify-icon>2025年AIGC年度回顾<!----></a></li></ul></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="AI全栈开发"><span class="title"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="cpu" width="1em" height="1em"></iconify-icon>AI全栈开发</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="AI增强型全栈开发者" class="vp-link nav-link nav-link" href="/ai-fullstack/AI%E5%A2%9E%E5%BC%BA%E5%9E%8B%E5%85%A8%E6%A0%88%E5%BC%80%E5%8F%91%E8%80%85%EF%BC%9A2026%E5%B9%B4%E6%9C%80%E5%85%B7%E7%AB%9E%E4%BA%89%E5%8A%9B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%A7%92%E8%89%B2.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="robot" width="1em" height="1em"></iconify-icon>AI增强型全栈开发者<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="前端开发"><span class="title"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="code" width="1em" height="1em"></iconify-icon>前端开发</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>面试资料</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="Vue篇" class="vp-link nav-link nav-link" href="/frontend/2023%E5%B9%B4%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-vue%E7%AF%87.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="vuejs" width="1em" height="1em"></iconify-icon>Vue篇<!----></a></li><li class="dropdown-subitem"><a aria-label="JS篇" class="vp-link nav-link nav-link" href="/frontend/2023%E5%B9%B4%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-JS%E7%AF%87.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="javascript" width="1em" height="1em"></iconify-icon>JS篇<!----></a></li><li class="dropdown-subitem"><a aria-label="HTML&amp;CSS篇" class="vp-link nav-link nav-link" href="/frontend/2023%E5%B9%B4%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-HTML_CSS%E7%AF%87.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="html5" width="1em" height="1em"></iconify-icon>HTML&amp;CSS篇<!----></a></li><li class="dropdown-subitem"><a aria-label="TypeScript篇" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-TypeScript%E7%AF%87.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="code" width="1em" height="1em"></iconify-icon>TypeScript篇<!----></a></li><li class="dropdown-subitem"><a aria-label="性能优化篇" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%AF%87.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="rocket" width="1em" height="1em"></iconify-icon>性能优化篇<!----></a></li><li class="dropdown-subitem"><a aria-label="工程化篇" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-%E5%B7%A5%E7%A8%8B%E5%8C%96%E7%AF%87.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tools" width="1em" height="1em"></iconify-icon>工程化篇<!----></a></li><li class="dropdown-subitem"><a aria-label="50道CSS基础面试题" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-50%E9%81%93CSS%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E9%99%84%E7%AD%94%E6%A1%88%EF%BC%89.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="css3-alt" width="1em" height="1em"></iconify-icon>50道CSS基础面试题<!----></a></li><li class="dropdown-subitem"><a aria-label="HTML 5 语义化" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-HTML%205%20%E8%AF%AD%E4%B9%89%E5%8C%96.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="file-code" width="1em" height="1em"></iconify-icon>HTML 5 语义化<!----></a></li></ul></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>技术学习</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="AST抽象语法树" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-AST%E6%8A%BD%E8%B1%A1%E8%AF%AD%E6%B3%95%E6%A0%91.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tree" width="1em" height="1em"></iconify-icon>AST抽象语法树<!----></a></li><li class="dropdown-subitem"><a aria-label="POST请求发送两次的原因" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-POST%E8%AF%B7%E6%B1%82%E5%8F%91%E9%80%81%E4%B8%A4%E6%AC%A1%E7%9A%84%E5%8E%9F%E5%9B%A0.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="question-circle" width="1em" height="1em"></iconify-icon>POST请求发送两次的原因<!----></a></li><li class="dropdown-subitem"><a aria-label="Vue+Axios全局接口防抖节流封装" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-Vue_Axios%E5%85%A8%E5%B1%80%E6%8E%A5%E5%8F%A3%E9%98%B2%E6%8A%96%E8%8A%82%E6%B5%81%E5%B0%81%E8%A3%85.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="shield-check" width="1em" height="1em"></iconify-icon>Vue+Axios全局接口防抖节流封装<!----></a></li><li class="dropdown-subitem"><a aria-label="浏览器渲染原理" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-%E6%B5%8F%E8%A7%88%E5%99%A8%E6%B8%B2%E6%9F%93%E5%8E%9F%E7%90%86.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="monitor" width="1em" height="1em"></iconify-icon>浏览器渲染原理<!----></a></li><li class="dropdown-subitem"><a aria-label="HTTP协议详解" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-HTTP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="network-wired" width="1em" height="1em"></iconify-icon>HTTP协议详解<!----></a></li><li class="dropdown-subitem"><a aria-label="跨域问题详解" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="globe" width="1em" height="1em"></iconify-icon>跨域问题详解<!----></a></li><li class="dropdown-subitem"><a aria-label="Web安全详解" class="vp-link nav-link nav-link" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-Web%E5%AE%89%E5%85%A8.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="shield-alt" width="1em" height="1em"></iconify-icon>Web安全详解<!----></a></li></ul></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="数据结构与算法"><span class="title"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="function" width="1em" height="1em"></iconify-icon>数据结构与算法</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="前言" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%951-%E5%89%8D%E8%A8%80.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="list-ol" width="1em" height="1em"></iconify-icon>前言<!----></a></li><li class="dropdown-item"><a aria-label="数组" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%952-%E6%95%B0%E7%BB%84.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="database" width="1em" height="1em"></iconify-icon>数组<!----></a></li><li class="dropdown-item"><a aria-label="二维数组" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%953-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="table" width="1em" height="1em"></iconify-icon>二维数组<!----></a></li><li class="dropdown-item"><a aria-label="链表" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E9%93%BE%E8%A1%A8.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="link" width="1em" height="1em"></iconify-icon>链表<!----></a></li><li class="dropdown-item"><a aria-label="栈与队列" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%955-%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="layer-group" width="1em" height="1em"></iconify-icon>栈与队列<!----></a></li><li class="dropdown-item"><a aria-label="树" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%956-%E6%A0%91.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="sitemap" width="1em" height="1em"></iconify-icon>树<!----></a></li><li class="dropdown-item"><a aria-label="排序算法" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%957-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="sort" width="1em" height="1em"></iconify-icon>排序算法<!----></a></li><li class="dropdown-item"><a aria-label="查找算法" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%958-%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="search" width="1em" height="1em"></iconify-icon>查找算法<!----></a></li><li class="dropdown-item"><a aria-label="动态规划" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%959-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="brain" width="1em" height="1em"></iconify-icon>动态规划<!----></a></li><li class="dropdown-item"><a aria-label="贪心算法" class="vp-link nav-link nav-link" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%9510-%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="bolt" width="1em" height="1em"></iconify-icon>贪心算法<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="游戏开发"><span class="title"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="gamepad" width="1em" height="1em"></iconify-icon>游戏开发</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="Unity是什么" class="vp-link nav-link nav-link" href="/game/Unity%E6%98%AF%E4%BB%80%E4%B9%88.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="controller" width="1em" height="1em"></iconify-icon>Unity是什么<!----></a></li><li class="dropdown-item"><a aria-label="Unreal Engine是什么" class="vp-link nav-link nav-link" href="/game/Unreal%20Engine%E6%98%AF%E4%BB%80%E4%B9%88.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="cube" width="1em" height="1em"></iconify-icon>Unreal Engine是什么<!----></a></li><li class="dropdown-item"><a aria-label="Godot是什么" class="vp-link nav-link nav-link" href="/game/Godot%E6%98%AF%E4%BB%80%E4%B9%88.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="dice" width="1em" height="1em"></iconify-icon>Godot是什么<!----></a></li><li class="dropdown-item"><a aria-label="打地鼠" class="vp-link nav-link nav-link" href="/game/%E6%89%93%E5%9C%B0%E9%BC%A0.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="game-controller" width="1em" height="1em"></iconify-icon>打地鼠<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="物联网"><span class="title"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="cloud" width="1em" height="1em"></iconify-icon>物联网</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="什么是物联网" class="vp-link nav-link nav-link" href="/thingsboard/%E4%BB%80%E4%B9%88%E6%98%AF%E7%89%A9%E8%81%94%E7%BD%91.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="wifi" width="1em" height="1em"></iconify-icon>什么是物联网<!----></a></li><li class="dropdown-item"><a aria-label="什么是ThingsBoard" class="vp-link nav-link nav-link" href="/thingsboard/%E4%BB%80%E4%B9%88%E6%98%AFThingsBoard.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="server" width="1em" height="1em"></iconify-icon>什么是ThingsBoard<!----></a></li><li class="dropdown-item"><a aria-label="基于MQTT的RPC协议" class="vp-link nav-link nav-link" href="/thingsboard/%E5%9F%BA%E4%BA%8EMQTT%E7%9A%84RPC%E5%8D%8F%E8%AE%AE.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="network-wired" width="1em" height="1em"></iconify-icon>基于MQTT的RPC协议<!----></a></li><li class="dropdown-item"><a aria-label="在Ubuntu（Linux）中部署ThingsBoard" class="vp-link nav-link nav-link" href="/thingsboard/%E5%9C%A8Ubuntu%EF%BC%88Linux%EF%BC%89%E4%B8%AD%E9%83%A8%E7%BD%B2ThingsBoard.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="ubuntu" width="1em" height="1em"></iconify-icon>在Ubuntu（Linux）中部署ThingsBoard<!----></a></li><li class="dropdown-item"><a aria-label="在Windows10中部署ThingsBoard" class="vp-link nav-link nav-link" href="/thingsboard/%E5%9C%A8Windows10%E4%B8%AD%E9%83%A8%E7%BD%B2ThingsBoard.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="windows" width="1em" height="1em"></iconify-icon>在Windows10中部署ThingsBoard<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="兴趣爱好"><span class="title"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="heart" width="1em" height="1em"></iconify-icon>兴趣爱好</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>随笔</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="笔记" class="vp-link nav-link nav-link" href="/blogs/notes/page.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="book" width="1em" height="1em"></iconify-icon>笔记<!----></a></li></ul></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>摄影</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="摄影" class="vp-link nav-link nav-link" href="/blogs/photography/page.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="image" width="1em" height="1em"></iconify-icon>摄影<!----></a></li></ul></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>音乐</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="音乐" class="vp-link nav-link nav-link" href="/blogs/music/page.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="headphones" width="1em" height="1em"></iconify-icon>音乐<!----></a></li></ul></li></ul></button></div></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><!----><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><!--[--><a aria-label="首页" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>首页<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="robot" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">AIGC</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="note" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">本周AIGC资讯</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="第2期" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/aigc/AI%E5%BA%94%E7%94%A8/%E6%9C%AC%E5%91%A8AIGC%E8%B5%84%E8%AE%AF-%E7%AC%AC2%E6%9C%9F.html"><!---->第2期<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="第1期" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/aigc/AI%E5%BA%94%E7%94%A8/%E6%9C%AC%E5%91%A8AIGC%E8%B5%84%E8%AE%AF-%E7%AC%AC1%E6%9C%9F.html"><!---->第1期<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="star" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">年度回顾</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="2025年AIGC年度回顾" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/aigc/AI%E5%BA%94%E7%94%A8/2025%E5%B9%B4AIGC%E5%B9%B4%E5%BA%A6%E5%9B%9E%E9%A1%BE%E7%89%B9%E5%88%8A.html"><!---->2025年AIGC年度回顾<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="cpu" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">AI全栈开发</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="AI增强型全栈开发者" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/ai-fullstack/AI%E5%A2%9E%E5%BC%BA%E5%9E%8B%E5%85%A8%E6%A0%88%E5%BC%80%E5%8F%91%E8%80%85%EF%BC%9A2026%E5%B9%B4%E6%9C%80%E5%85%B7%E7%AB%9E%E4%BA%89%E5%8A%9B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%A7%92%E8%89%B2.html"><!---->AI增强型全栈开发者<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="code" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">前端开发</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="note" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">面试资料</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="Vue篇" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/2023%E5%B9%B4%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-vue%E7%AF%87.html"><!---->Vue篇<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="JS篇" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/2023%E5%B9%B4%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-JS%E7%AF%87.html"><!---->JS篇<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="HTML&amp;CSS篇" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/2023%E5%B9%B4%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-HTML_CSS%E7%AF%87.html"><!---->HTML&amp;CSS篇<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="TypeScript篇" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-TypeScript%E7%AF%87.html"><!---->TypeScript篇<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="性能优化篇" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%AF%87.html"><!---->性能优化篇<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="工程化篇" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-%E5%B7%A5%E7%A8%8B%E5%8C%96%E7%AF%87.html"><!---->工程化篇<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="50道CSS基础面试题" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-50%E9%81%93CSS%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E9%99%84%E7%AD%94%E6%A1%88%EF%BC%89.html"><!---->50道CSS基础面试题<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="HTML 5 语义化" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97-HTML%205%20%E8%AF%AD%E4%B9%89%E5%8C%96.html"><!---->HTML 5 语义化<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="note" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">技术学习</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="AST抽象语法树" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-AST%E6%8A%BD%E8%B1%A1%E8%AF%AD%E6%B3%95%E6%A0%91.html"><!---->AST抽象语法树<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="POST请求发送两次的原因" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-POST%E8%AF%B7%E6%B1%82%E5%8F%91%E9%80%81%E4%B8%A4%E6%AC%A1%E7%9A%84%E5%8E%9F%E5%9B%A0.html"><!---->POST请求发送两次的原因<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Vue+Axios全局接口防抖节流封装" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-Vue_Axios%E5%85%A8%E5%B1%80%E6%8E%A5%E5%8F%A3%E9%98%B2%E6%8A%96%E8%8A%82%E6%B5%81%E5%B0%81%E8%A3%85.html"><!---->Vue+Axios全局接口防抖节流封装<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="浏览器渲染原理" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-%E6%B5%8F%E8%A7%88%E5%99%A8%E6%B8%B2%E6%9F%93%E5%8E%9F%E7%90%86.html"><!---->浏览器渲染原理<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="HTTP协议详解" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-HTTP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3.html"><!---->HTTP协议详解<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="跨域问题详解" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3.html"><!---->跨域问题详解<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Web安全详解" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/frontend/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0-Web%E5%AE%89%E5%85%A8.html"><!---->Web安全详解<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="function" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">数据结构与算法</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="前言" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%951-%E5%89%8D%E8%A8%80.html"><!---->前言<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="数组" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%952-%E6%95%B0%E7%BB%84.html"><!---->数组<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="二维数组" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%953-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84.html"><!---->二维数组<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="链表" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E9%93%BE%E8%A1%A8.html"><!---->链表<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="栈与队列" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%955-%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97.html"><!---->栈与队列<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="树" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%956-%E6%A0%91.html"><!---->树<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="排序算法" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%957-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95.html"><!---->排序算法<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="查找算法" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%958-%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95.html"><!---->查找算法<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="动态规划" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%959-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92.html"><!---->动态规划<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="贪心算法" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%9510-%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95.html"><!---->贪心算法<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="gamepad" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">游戏开发</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="Unity是什么" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/game/Unity%E6%98%AF%E4%BB%80%E4%B9%88.html"><!---->Unity是什么<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Unreal Engine是什么" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/game/Unreal%20Engine%E6%98%AF%E4%BB%80%E4%B9%88.html"><!---->Unreal Engine是什么<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Godot是什么" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/game/Godot%E6%98%AF%E4%BB%80%E4%B9%88.html"><!---->Godot是什么<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="打地鼠" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/game/%E6%89%93%E5%9C%B0%E9%BC%A0.html"><!---->打地鼠<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="link" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">物联网</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="什么是物联网" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/thingsboard/%E4%BB%80%E4%B9%88%E6%98%AF%E7%89%A9%E8%81%94%E7%BD%91.html"><!---->什么是物联网<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="什么是ThingsBoard" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/thingsboard/%E4%BB%80%E4%B9%88%E6%98%AFThingsBoard.html"><!---->什么是ThingsBoard<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="基于MQTT的RPC协议" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/thingsboard/%E5%9F%BA%E4%BA%8EMQTT%E7%9A%84RPC%E5%8D%8F%E8%AE%AE.html"><!---->基于MQTT的RPC协议<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="在Ubuntu（Linux）中部署ThingsBoard" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/thingsboard/%E5%9C%A8Ubuntu%EF%BC%88Linux%EF%BC%89%E4%B8%AD%E9%83%A8%E7%BD%B2ThingsBoard.html"><!---->在Ubuntu（Linux）中部署ThingsBoard<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="在Windows10中部署ThingsBoard" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/thingsboard/%E5%9C%A8Windows10%E4%B8%AD%E9%83%A8%E7%BD%B2ThingsBoard.html"><!---->在Windows10中部署ThingsBoard<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="heart" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">兴趣爱好</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="note" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">随笔</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="笔记" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/notes/page.html"><!---->笔记<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="note" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">摄影</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="摄影" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/photography/page.html"><!---->摄影<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="note" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">音乐</span><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="音乐" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/music/page.html"><!---->音乐<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li></ul></section></li><li><!--[--><a aria-label="关于我" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/intro.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="info" width="1em" height="1em"></iconify-icon>关于我<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Slide page" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/slides.html"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="slides" width="1em" height="1em"></iconify-icon>Slide page<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><iconify-icon class="font-icon icon" style="" mode="style" inline icon="edit" width="1em" height="1em"></iconify-icon>2025年大模型部署框架全解析：vLLM vs Ollama</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://www.baidu.com" target="_blank" rel="noopener noreferrer">Kevin</a></span><span property="author" content="Kevin"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2025-12-26T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 9 分钟</span><meta property="timeRequired" content="PT9M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category2 clickable" role="navigation">AI框架</span><!--]--><meta property="articleSection" content="AI框架"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag7 clickable" role="navigation">vLLM</span><span class="page-tag-item tag3 clickable" role="navigation">Ollama</span><span class="page-tag-item tag5 clickable" role="navigation">部署</span><!--]--><meta property="keywords" content="vLLM,Ollama,部署"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#引言">引言</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#框架概述">框架概述</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#vllm-高性能推理引擎">vLLM：高性能推理引擎</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#ollama-本地部署利器">Ollama：本地部署利器</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#技术架构对比">技术架构对比</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#vllm架构">vLLM架构</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#ollama架构">Ollama架构</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#功能对比">功能对比</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-模型支持">1. 模型支持</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-部署方式">2. 部署方式</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-性能对比">3. 性能对比</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-资源需求">4. 资源需求</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#应用场景">应用场景</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#vllm适用场景">vLLM适用场景</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#ollama适用场景">Ollama适用场景</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#最佳实践">最佳实践</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#vllm最佳实践">vLLM最佳实践</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#ollama最佳实践">Ollama最佳实践</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#选型建议">选型建议</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#决策树">决策树</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#推荐方案">推荐方案</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#性能优化">性能优化</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#vllm优化技巧">vLLM优化技巧</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#ollama优化技巧">Ollama优化技巧</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#监控与调试">监控与调试</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#vllm监控">vLLM监控</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#ollama监控">Ollama监控</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#故障排查">故障排查</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#常见问题">常见问题</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#未来展望">未来展望</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#vllm发展方向">vLLM发展方向</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#ollama发展方向">Ollama发展方向</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#总结">总结</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="_2025年大模型部署框架全解析-vllm-vs-ollama" tabindex="-1"><a class="header-anchor" href="#_2025年大模型部署框架全解析-vllm-vs-ollama" aria-hidden="true">#</a> 2025年大模型部署框架全解析：vLLM vs Ollama</h1><blockquote><p>从个人实验到企业生产，选择最适合你的部署方案</p></blockquote><h2 id="引言" tabindex="-1"><a class="header-anchor" href="#引言" aria-hidden="true">#</a> 引言</h2><p>2025年，大模型本地部署已成为AI应用落地的关键环节。随着vLLM和Ollama等开源框架的成熟，开发者可以轻松在本地运行各类开源大语言模型。本文将深入对比这两大主流部署框架，帮助你选择最适合的方案。</p><h2 id="框架概述" tabindex="-1"><a class="header-anchor" href="#框架概述" aria-hidden="true">#</a> 框架概述</h2><h3 id="vllm-高性能推理引擎" tabindex="-1"><a class="header-anchor" href="#vllm-高性能推理引擎" aria-hidden="true">#</a> vLLM：高性能推理引擎</h3><p><strong>定位：</strong> 企业级、高性能推理服务</p><p><strong>背景：</strong></p><ul><li>开发者：UC Berkeley SkyPilot团队</li><li>核心技术：PagedAttention、张量并行、流水线并行</li><li>目标：解决大模型服务中的显存效率与吞吐量瓶颈</li></ul><p><strong>核心优势：</strong></p><ul><li>超高吞吐量</li><li>低延迟响应</li><li>支持多模型并发</li><li>企业级稳定性</li></ul><h3 id="ollama-本地部署利器" tabindex="-1"><a class="header-anchor" href="#ollama-本地部署利器" aria-hidden="true">#</a> Ollama：本地部署利器</h3><p><strong>定位：</strong> 个人开发者、轻量化场景</p><p><strong>背景：</strong></p><ul><li>开发者：Ollama团队（原GitHub Copilot核心成员）</li><li>核心技术：llama.cpp、GGUF格式</li><li>目标：让开发者轻松本地运行大模型</li></ul><p><strong>核心优势：</strong></p><ul><li>开箱即用</li><li>跨平台支持</li><li>简单易用</li><li>轻量级部署</li></ul><h2 id="技术架构对比" tabindex="-1"><a class="header-anchor" href="#技术架构对比" aria-hidden="true">#</a> 技术架构对比</h2><h3 id="vllm架构" tabindex="-1"><a class="header-anchor" href="#vllm架构" aria-hidden="true">#</a> vLLM架构</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">vLLMArchitecture</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># PagedAttention：分页缓存机制</span>
        self<span class="token punctuation">.</span>paged_attention <span class="token operator">=</span> PagedAttention<span class="token punctuation">(</span>
            block_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
            num_gpu_blocks<span class="token operator">=</span><span class="token number">1024</span>
        <span class="token punctuation">)</span>
        
        <span class="token comment"># 连续批处理</span>
        self<span class="token punctuation">.</span>continuous_batching <span class="token operator">=</span> ContinuousBatching<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 张量并行</span>
        self<span class="token punctuation">.</span>tensor_parallel <span class="token operator">=</span> TensorParallel<span class="token punctuation">(</span>
            world_size<span class="token operator">=</span><span class="token number">8</span>
        <span class="token punctuation">)</span>
        
        <span class="token comment"># 流水线并行</span>
        self<span class="token punctuation">.</span>pipeline_parallel <span class="token operator">=</span> PipelineParallel<span class="token punctuation">(</span>
            num_stages<span class="token operator">=</span><span class="token number">4</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>关键技术：</strong></p><h4 id="_1-pagedattention" tabindex="-1"><a class="header-anchor" href="#_1-pagedattention" aria-hidden="true">#</a> 1. PagedAttention</h4><p><strong>原理：</strong></p><ul><li>将KV Cache分块管理</li><li>按需分配和释放显存</li><li>支持超长上下文</li></ul><p><strong>优势：</strong></p><ul><li>显存利用率提升2-3倍</li><li>支持上万个并发请求</li><li>显存碎片化减少</li></ul><h4 id="_2-连续批处理" tabindex="-1"><a class="header-anchor" href="#_2-连续批处理" aria-hidden="true">#</a> 2. 连续批处理</h4><p><strong>功能：</strong></p><ul><li>动态批处理不同长度的请求</li><li>最大化GPU利用率</li><li>减少推理延迟</li></ul><p><strong>代码示例：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM<span class="token punctuation">,</span> SamplingParams

<span class="token comment"># 创建LLM实例</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    tensor_parallel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    gpu_memory_utilization<span class="token operator">=</span><span class="token number">0.9</span>
<span class="token punctuation">)</span>

<span class="token comment"># 创建采样参数</span>
sampling_params <span class="token operator">=</span> SamplingParams<span class="token punctuation">(</span>
    temperature<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span>
    top_p<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">,</span>
    max_tokens<span class="token operator">=</span><span class="token number">1000</span>
<span class="token punctuation">)</span>

<span class="token comment"># 推理</span>
outputs <span class="token operator">=</span> llm<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
    prompts<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;解释量子计算&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;什么是机器学习&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    sampling_params<span class="token operator">=</span>sampling_params
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ollama架构" tabindex="-1"><a class="header-anchor" href="#ollama架构" aria-hidden="true">#</a> Ollama架构</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">OllamaArchitecture</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># llama.cpp核心</span>
        self<span class="token punctuation">.</span>llama_cpp <span class="token operator">=</span> LlamaCppEngine<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># GGUF格式支持</span>
        self<span class="token punctuation">.</span>gguf_loader <span class="token operator">=</span> GGUFLoader<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 模型管理</span>
        self<span class="token punctuation">.</span>model_manager <span class="token operator">=</span> ModelManager<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># API服务</span>
        self<span class="token punctuation">.</span>api_server <span class="token operator">=</span> APIServer<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>关键技术：</strong></p><h4 id="_1-gguf格式" tabindex="-1"><a class="header-anchor" href="#_1-gguf格式" aria-hidden="true">#</a> 1. GGUF格式</h4><p><strong>特点：</strong></p><ul><li>压缩的模型权重</li><li>支持量化（INT4、INT8）</li><li>快速加载</li></ul><p><strong>优势：</strong></p><ul><li>模型大小减少50-75%</li><li>加载速度提升3-5倍</li><li>内存占用大幅降低</li></ul><h4 id="_2-模块化设计" tabindex="-1"><a class="header-anchor" href="#_2-模块化设计" aria-hidden="true">#</a> 2. 模块化设计</h4><p><strong>架构：</strong></p><div class="language-go line-numbers-mode" data-ext="go"><pre class="language-go"><code><span class="token comment">// 模块化组件</span>
<span class="token keyword">type</span> Ollama <span class="token keyword">struct</span> <span class="token punctuation">{</span>
    Model      ModelEngine
    Server     ServerEngine
    API        APIEngine
    CLI        CLIEngine
<span class="token punctuation">}</span>

<span class="token comment">// 模型引擎</span>
<span class="token keyword">type</span> ModelEngine <span class="token keyword">struct</span> <span class="token punctuation">{</span>
    GGUFLoader GGUFLoader
    Inference  InferenceEngine
    Cache      ModelCache
<span class="token punctuation">}</span>

<span class="token comment">// 推理引擎</span>
<span class="token keyword">type</span> InferenceEngine <span class="token keyword">struct</span> <span class="token punctuation">{</span>
    Context    InferenceContext
    Generator  TextGenerator
    Sampler    TokenSampler
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="功能对比" tabindex="-1"><a class="header-anchor" href="#功能对比" aria-hidden="true">#</a> 功能对比</h2><h3 id="_1-模型支持" tabindex="-1"><a class="header-anchor" href="#_1-模型支持" aria-hidden="true">#</a> 1. 模型支持</h3><table><thead><tr><th>特性</th><th>vLLM</th><th>Ollama</th></tr></thead><tbody><tr><td>支持模型</td><td>Hugging Face所有模型（fp16/bf16）</td><td>GGUF格式模型（Llama、Qwen、Mistral等）</td></tr><tr><td>模型格式</td><td>PyTorch、Safetensors</td><td>GGUF</td></tr><tr><td>模型量化</td><td>FP16、BF16、INT8</td><td>INT4、INT8、FP16</td></tr><tr><td>自定义模型</td><td>需要转换为PyTorch格式</td><td>支持自定义GGUF模型</td></tr></tbody></table><h3 id="_2-部署方式" tabindex="-1"><a class="header-anchor" href="#_2-部署方式" aria-hidden="true">#</a> 2. 部署方式</h3><h4 id="vllm部署" tabindex="-1"><a class="header-anchor" href="#vllm部署" aria-hidden="true">#</a> vLLM部署</h4><p><strong>方式1：Python API</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM<span class="token punctuation">,</span> SamplingParams

llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">)</span>
outputs <span class="token operator">=</span> llm<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>prompts<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;你好&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampling_params<span class="token operator">=</span>SamplingParams<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>方式2：OpenAI兼容API</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 启动vLLM服务器</span>
python <span class="token parameter variable">-m</span> vllm.entrypoints.openai.api_server <span class="token punctuation">\</span>
    <span class="token parameter variable">--model</span> meta-llama/Llama-3-8B <span class="token punctuation">\</span>
    <span class="token parameter variable">--port</span> <span class="token number">8000</span>

<span class="token comment"># 调用API</span>
<span class="token function">curl</span> http://localhost:8000/v1/completions <span class="token punctuation">\</span>
  <span class="token parameter variable">-H</span> <span class="token string">&quot;Content-Type: application/json&quot;</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-d</span> <span class="token string">&#39;{
    &quot;model&quot;: &quot;meta-llama/Llama-3-8B&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;}]
  }&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>方式3：Docker部署</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 拉取镜像</span>
<span class="token function">docker</span> pull vllm/vllm-openai:latest

<span class="token comment"># 运行容器</span>
<span class="token function">docker</span> run <span class="token parameter variable">--gpus</span> all <span class="token punctuation">\</span>
    <span class="token parameter variable">-p</span> <span class="token number">8000</span>:8000 <span class="token punctuation">\</span>
    --shm-size<span class="token operator">=</span>10g <span class="token punctuation">\</span>
    vllm/vllm-openai:latest <span class="token punctuation">\</span>
    <span class="token parameter variable">--model</span> meta-llama/Llama-3-8B
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="ollama部署" tabindex="-1"><a class="header-anchor" href="#ollama部署" aria-hidden="true">#</a> Ollama部署</h4><p><strong>方式1：命令行交互</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 安装Ollama</span>
<span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://ollama.com/install.sh <span class="token operator">|</span> <span class="token function">sh</span>

<span class="token comment"># 拉取模型</span>
ollama pull llama3:8b

<span class="token comment"># 运行模型</span>
ollama run llama3:8b <span class="token string">&quot;你好&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>方式2：API服务</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 启动API服务</span>
ollama serve

<span class="token comment"># 调用API</span>
<span class="token function">curl</span> http://localhost:11434/api/generate <span class="token punctuation">\</span>
  <span class="token parameter variable">-d</span> <span class="token string">&#39;{
    &quot;model&quot;: &quot;llama3:8b&quot;,
    &quot;prompt&quot;: &quot;你好&quot;
  }&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>方式3：Python SDK</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> ollama

<span class="token comment"># 加载模型</span>
llm <span class="token operator">=</span> ollama<span class="token punctuation">.</span>pull<span class="token punctuation">(</span><span class="token string">&quot;llama3:8b&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 生成文本</span>
response <span class="token operator">=</span> ollama<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;llama3:8b&quot;</span><span class="token punctuation">,</span>
    prompt<span class="token operator">=</span><span class="token string">&quot;你好&quot;</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-性能对比" tabindex="-1"><a class="header-anchor" href="#_3-性能对比" aria-hidden="true">#</a> 3. 性能对比</h3><h4 id="吞吐量对比" tabindex="-1"><a class="header-anchor" href="#吞吐量对比" aria-hidden="true">#</a> 吞吐量对比</h4><table><thead><tr><th>模型</th><th>vLLM</th><th>Ollama</th></tr></thead><tbody><tr><td>Llama 3-8B</td><td>500+ tokens/s</td><td>50-100 tokens/s</td></tr><tr><td>Llama 3-70B</td><td>200+ tokens/s</td><td>20-40 tokens/s</td></tr><tr><td>Qwen3-72B</td><td>300+ tokens/s</td><td>30-60 tokens/s</td></tr></tbody></table><p><strong>测试环境：</strong></p><ul><li>GPU: NVIDIA A100 80GB</li><li>Batch Size: 1</li><li>Max Tokens: 1000</li></ul><h4 id="延迟对比" tabindex="-1"><a class="header-anchor" href="#延迟对比" aria-hidden="true">#</a> 延迟对比</h4><table><thead><tr><th>场景</th><th>vLLM</th><th>Ollama</th></tr></thead><tbody><tr><td>首字延迟</td><td>&lt;100ms</td><td>200-500ms</td></tr><tr><td>平均延迟</td><td>50-100ms</td><td>100-300ms</td></tr><tr><td>P99延迟</td><td>200-500ms</td><td>500-1000ms</td></tr></tbody></table><h4 id="并发能力" tabindex="-1"><a class="header-anchor" href="#并发能力" aria-hidden="true">#</a> 并发能力</h4><table><thead><tr><th>并发数</th><th>vLLM</th><th>Ollama</th></tr></thead><tbody><tr><td>10</td><td>稳定</td><td>稳定</td></tr><tr><td>50</td><td>稳定</td><td>延迟增加</td></tr><tr><td>100</td><td>稳定</td><td>可能崩溃</td></tr><tr><td>1000</td><td>稳定</td><td>不支持</td></tr></tbody></table><h3 id="_4-资源需求" tabindex="-1"><a class="header-anchor" href="#_4-资源需求" aria-hidden="true">#</a> 4. 资源需求</h3><h4 id="硬件需求" tabindex="-1"><a class="header-anchor" href="#硬件需求" aria-hidden="true">#</a> 硬件需求</h4><table><thead><tr><th>模型</th><th>vLLM</th><th>Ollama</th></tr></thead><tbody><tr><td>Llama 3-8B</td><td>16GB</td><td>8GB (INT4)</td></tr><tr><td>Llama 3-70B</td><td>140GB</td><td>70GB (INT4)</td></tr><tr><td>Qwen3-72B</td><td>144GB</td><td>72GB (INT4)</td></tr></tbody></table><h4 id="内存占用" tabindex="-1"><a class="header-anchor" href="#内存占用" aria-hidden="true">#</a> 内存占用</h4><table><thead><tr><th>模型</th><th>vLLM (FP16)</th><th>Ollama (INT4)</th></tr></thead><tbody><tr><td>Llama 3-8B</td><td>16GB</td><td>6GB</td></tr><tr><td>Llama 3-70B</td><td>140GB</td><td>38GB</td></tr><tr><td>Qwen3-72B</td><td>144GB</td><td>40GB</td></tr></tbody></table><h2 id="应用场景" tabindex="-1"><a class="header-anchor" href="#应用场景" aria-hidden="true">#</a> 应用场景</h2><h3 id="vllm适用场景" tabindex="-1"><a class="header-anchor" href="#vllm适用场景" aria-hidden="true">#</a> vLLM适用场景</h3><h4 id="_1-企业级服务" tabindex="-1"><a class="header-anchor" href="#_1-企业级服务" aria-hidden="true">#</a> 1. 企业级服务</h4><p><strong>场景：</strong></p><ul><li>高并发API服务</li><li>实时对话系统</li><li>批量文档处理</li></ul><p><strong>优势：</strong></p><ul><li>高吞吐量</li><li>低延迟</li><li>稳定可靠</li></ul><h4 id="_2-多模型服务" tabindex="-1"><a class="header-anchor" href="#_2-多模型服务" aria-hidden="true">#</a> 2. 多模型服务</h4><p><strong>场景：</strong></p><ul><li>同时服务多个模型</li><li>模型A/B测试</li><li>动态模型切换</li></ul><p><strong>实现：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM<span class="token punctuation">,</span> SamplingParams

<span class="token comment"># 加载多个模型</span>
llama <span class="token operator">=</span> LLM<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">)</span>
qwen <span class="token operator">=</span> LLM<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">&quot;Qwen/Qwen2.5-7B&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 创建采样参数</span>
sampling_params <span class="token operator">=</span> SamplingParams<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">)</span>

<span class="token comment"># 并发推理</span>
results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> prompt <span class="token keyword">in</span> prompts<span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">&quot;中文&quot;</span> <span class="token keyword">in</span> prompt<span class="token punctuation">:</span>
        results<span class="token punctuation">.</span>append<span class="token punctuation">(</span>qwen<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token punctuation">[</span>prompt<span class="token punctuation">]</span><span class="token punctuation">,</span> sampling_params<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        results<span class="token punctuation">.</span>append<span class="token punctuation">(</span>llama<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token punctuation">[</span>prompt<span class="token punctuation">]</span><span class="token punctuation">,</span> sampling_params<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-分布式部署" tabindex="-1"><a class="header-anchor" href="#_3-分布式部署" aria-hidden="true">#</a> 3. 分布式部署</h4><p><strong>场景：</strong></p><ul><li>大规模集群部署</li><li>超大模型推理</li><li>负载均衡</li></ul><p><strong>架构：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM

<span class="token comment"># 分布式推理</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-405B&quot;</span><span class="token punctuation">,</span>
    tensor_parallel_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    pipeline_parallel_size<span class="token operator">=</span><span class="token number">4</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ollama适用场景" tabindex="-1"><a class="header-anchor" href="#ollama适用场景" aria-hidden="true">#</a> Ollama适用场景</h3><h4 id="_1-个人开发实验" tabindex="-1"><a class="header-anchor" href="#_1-个人开发实验" aria-hidden="true">#</a> 1. 个人开发实验</h4><p><strong>场景：</strong></p><ul><li>快速体验新模型</li><li>本地测试和调试</li><li>学习和研究</li></ul><p><strong>优势：</strong></p><ul><li>一键部署</li><li>简单易用</li><li>资源占用低</li></ul><h4 id="_2-边缘设备部署" tabindex="-1"><a class="header-anchor" href="#_2-边缘设备部署" aria-hidden="true">#</a> 2. 边缘设备部署</h4><p><strong>场景：</strong></p><ul><li>树莓派部署</li><li>笔记本本地运行</li><li>离线环境</li></ul><p><strong>优势：</strong></p><ul><li>轻量级</li><li>跨平台</li><li>低功耗</li></ul><h4 id="_3-原型开发" tabindex="-1"><a class="header-anchor" href="#_3-原型开发" aria-hidden="true">#</a> 3. 原型开发</h4><p><strong>场景：</strong></p><ul><li>快速验证想法</li><li>概念验证（POC）</li><li>技术选型</li></ul><p><strong>优势：</strong></p><ul><li>快速迭代</li><li>低成本</li><li>易于调试</li></ul><h2 id="最佳实践" tabindex="-1"><a class="header-anchor" href="#最佳实践" aria-hidden="true">#</a> 最佳实践</h2><h3 id="vllm最佳实践" tabindex="-1"><a class="header-anchor" href="#vllm最佳实践" aria-hidden="true">#</a> vLLM最佳实践</h3><h4 id="_1-优化配置" tabindex="-1"><a class="header-anchor" href="#_1-优化配置" aria-hidden="true">#</a> 1. 优化配置</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM

<span class="token comment"># 优化配置</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    <span class="token comment"># 张量并行</span>
    tensor_parallel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    <span class="token comment"># GPU内存利用率</span>
    gpu_memory_utilization<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>
    <span class="token comment"># 最大模型长度</span>
    max_model_len<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span>
    <span class="token comment"># KV Cache设置</span>
    block_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
    <span class="token comment"># 启用CUDA图</span>
    enable_prefix_caching<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-监控与日志" tabindex="-1"><a class="header-anchor" href="#_2-监控与日志" aria-hidden="true">#</a> 2. 监控与日志</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm<span class="token punctuation">.</span>engine<span class="token punctuation">.</span>arg_utils <span class="token keyword">import</span> EngineArgs

<span class="token comment"># 启用详细日志</span>
engine_args <span class="token operator">=</span> EngineArgs<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    disable_log_stats<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    disable_log_requests<span class="token operator">=</span><span class="token boolean">False</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-错误处理" tabindex="-1"><a class="header-anchor" href="#_3-错误处理" aria-hidden="true">#</a> 3. 错误处理</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM<span class="token punctuation">,</span> SamplingParams
<span class="token keyword">import</span> logging

<span class="token comment"># 设置日志</span>
logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>

<span class="token keyword">try</span><span class="token punctuation">:</span>
    llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">)</span>
    outputs <span class="token operator">=</span> llm<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
        prompts<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;测试&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        sampling_params<span class="token operator">=</span>SamplingParams<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
<span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>
    logging<span class="token punctuation">.</span>error<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;推理失败: </span><span class="token interpolation"><span class="token punctuation">{</span>e<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    <span class="token comment"># 降级处理</span>
    outputs <span class="token operator">=</span> fallback_generate<span class="token punctuation">(</span>prompts<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ollama最佳实践" tabindex="-1"><a class="header-anchor" href="#ollama最佳实践" aria-hidden="true">#</a> Ollama最佳实践</h3><h4 id="_1-模型管理" tabindex="-1"><a class="header-anchor" href="#_1-模型管理" aria-hidden="true">#</a> 1. 模型管理</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 查看已安装模型</span>
ollama list

<span class="token comment"># 更新模型</span>
ollama pull llama3:8b

<span class="token comment"># 删除模型</span>
ollama <span class="token function">rm</span> llama3:8b

<span class="token comment"># 创建模型副本</span>
ollama <span class="token function">cp</span> llama3:8b my-model
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-参数调优" tabindex="-1"><a class="header-anchor" href="#_2-参数调优" aria-hidden="true">#</a> 2. 参数调优</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 调整温度</span>
ollama run llama3:8b <span class="token parameter variable">--temperature</span> <span class="token number">0.7</span> <span class="token string">&quot;你好&quot;</span>

<span class="token comment"># 调整Top-P</span>
ollama run llama3:8b <span class="token parameter variable">--top_p</span> <span class="token number">0.9</span> <span class="token string">&quot;你好&quot;</span>

<span class="token comment"># 设置最大token数</span>
ollama run llama3:8b <span class="token parameter variable">--num_ctx</span> <span class="token number">4096</span> <span class="token string">&quot;你好&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-api集成" tabindex="-1"><a class="header-anchor" href="#_3-api集成" aria-hidden="true">#</a> 3. API集成</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> requests

<span class="token comment"># 调用Ollama API</span>
response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>
    <span class="token string">&quot;http://localhost:11434/api/generate&quot;</span><span class="token punctuation">,</span>
    json<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;model&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;llama3:8b&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;prompt&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;你好&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;stream&quot;</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token string">&quot;options&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;temperature&quot;</span><span class="token punctuation">:</span> <span class="token number">0.7</span><span class="token punctuation">,</span>
            <span class="token string">&quot;num_predict&quot;</span><span class="token punctuation">:</span> <span class="token number">1000</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">)</span>

result <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">[</span><span class="token string">&quot;response&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="选型建议" tabindex="-1"><a class="header-anchor" href="#选型建议" aria-hidden="true">#</a> 选型建议</h2><h3 id="决策树" tabindex="-1"><a class="header-anchor" href="#决策树" aria-hidden="true">#</a> 决策树</h3><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>开始
  │
  ├─ 是否需要高并发服务？
  │   ├─ 是 → 选择vLLM
  │   └─ 否 → 继续
  │
  ├─ 是否是企业级生产环境？
  │   ├─ 是 → 选择vLLM
  │   └─ 否 → 继续
  │
  ├─ 是否需要快速原型开发？
  │   ├─ 是 → 选择Ollama
  │   └─ 否 → 继续
  │
  ├─ 硬件资源是否有限？
  │   ├─ 是 → 选择Ollama
  │   └─ 否 → 继续
  │
  └─ 综合需求 → 混合部署
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="推荐方案" tabindex="-1"><a class="header-anchor" href="#推荐方案" aria-hidden="true">#</a> 推荐方案</h3><h4 id="方案1-个人开发" tabindex="-1"><a class="header-anchor" href="#方案1-个人开发" aria-hidden="true">#</a> 方案1：个人开发</h4><p><strong>推荐：</strong> Ollama</p><p><strong>原因：</strong></p><ul><li>一键部署</li><li>简单易用</li><li>资源占用低</li></ul><p><strong>配置：</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 安装Ollama</span>
<span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://ollama.com/install.sh <span class="token operator">|</span> <span class="token function">sh</span>

<span class="token comment"># 拉取轻量模型</span>
ollama pull llama3:8b

<span class="token comment"># 运行</span>
ollama run llama3:8b
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="方案2-小型团队" tabindex="-1"><a class="header-anchor" href="#方案2-小型团队" aria-hidden="true">#</a> 方案2：小型团队</h4><p><strong>推荐：</strong> Ollama + vLLM混合</p><p><strong>架构：</strong></p><ul><li>开发环境：Ollama</li><li>测试环境：Ollama</li><li>生产环境：vLLM</li></ul><p><strong>配置：</strong></p><div class="language-yaml line-numbers-mode" data-ext="yml"><pre class="language-yaml"><code><span class="token comment"># 开发环境</span>
<span class="token key atrule">dev</span><span class="token punctuation">:</span>
  <span class="token key atrule">framework</span><span class="token punctuation">:</span> ollama
  <span class="token key atrule">model</span><span class="token punctuation">:</span> llama3<span class="token punctuation">-</span>8b
  <span class="token key atrule">quantization</span><span class="token punctuation">:</span> int4

<span class="token comment"># 生产环境</span>
<span class="token key atrule">prod</span><span class="token punctuation">:</span>
  <span class="token key atrule">framework</span><span class="token punctuation">:</span> vllm
  <span class="token key atrule">model</span><span class="token punctuation">:</span> meta<span class="token punctuation">-</span>llama/Llama<span class="token punctuation">-</span>3<span class="token punctuation">-</span>8B
  <span class="token key atrule">tensor_parallel_size</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">gpu_memory_utilization</span><span class="token punctuation">:</span> <span class="token number">0.9</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="方案3-企业级服务" tabindex="-1"><a class="header-anchor" href="#方案3-企业级服务" aria-hidden="true">#</a> 方案3：企业级服务</h4><p><strong>推荐：</strong> vLLM</p><p><strong>原因：</strong></p><ul><li>高吞吐量</li><li>低延迟</li><li>稳定可靠</li><li>企业级支持</li></ul><p><strong>配置：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM

<span class="token comment"># 生产级配置</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-70B&quot;</span><span class="token punctuation">,</span>
    tensor_parallel_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
    pipeline_parallel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    gpu_memory_utilization<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>
    max_model_len<span class="token operator">=</span><span class="token number">16384</span><span class="token punctuation">,</span>
    enable_prefix_caching<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    disable_log_stats<span class="token operator">=</span><span class="token boolean">False</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="性能优化" tabindex="-1"><a class="header-anchor" href="#性能优化" aria-hidden="true">#</a> 性能优化</h2><h3 id="vllm优化技巧" tabindex="-1"><a class="header-anchor" href="#vllm优化技巧" aria-hidden="true">#</a> vLLM优化技巧</h3><h4 id="_1-启用前缀缓存" tabindex="-1"><a class="header-anchor" href="#_1-启用前缀缓存" aria-hidden="true">#</a> 1. 启用前缀缓存</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    enable_prefix_caching<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-优化kv-cache" tabindex="-1"><a class="header-anchor" href="#_2-优化kv-cache" aria-hidden="true">#</a> 2. 优化KV Cache</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    block_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
    max_num_seqs<span class="token operator">=</span><span class="token number">256</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-使用flash-attention-2" tabindex="-1"><a class="header-anchor" href="#_3-使用flash-attention-2" aria-hidden="true">#</a> 3. 使用Flash Attention 2</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    enable_flash_attn<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ollama优化技巧" tabindex="-1"><a class="header-anchor" href="#ollama优化技巧" aria-hidden="true">#</a> Ollama优化技巧</h3><h4 id="_1-使用量化模型" tabindex="-1"><a class="header-anchor" href="#_1-使用量化模型" aria-hidden="true">#</a> 1. 使用量化模型</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 拉取INT4量化模型</span>
ollama pull llama3:8b-q4_0

<span class="token comment"># 运行量化模型</span>
ollama run llama3:8b-q4_0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-调整上下文长度" tabindex="-1"><a class="header-anchor" href="#_2-调整上下文长度" aria-hidden="true">#</a> 2. 调整上下文长度</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 设置较小的上下文长度</span>
ollama run llama3:8b <span class="token parameter variable">--num_ctx</span> <span class="token number">2048</span> <span class="token string">&quot;你好&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-使用gpu加速" tabindex="-1"><a class="header-anchor" href="#_3-使用gpu加速" aria-hidden="true">#</a> 3. 使用GPU加速</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 指定GPU</span>
<span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> ollama run llama3:8b
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="监控与调试" tabindex="-1"><a class="header-anchor" href="#监控与调试" aria-hidden="true">#</a> 监控与调试</h2><h3 id="vllm监控" tabindex="-1"><a class="header-anchor" href="#vllm监控" aria-hidden="true">#</a> vLLM监控</h3><h4 id="_1-性能指标" tabindex="-1"><a class="header-anchor" href="#_1-性能指标" aria-hidden="true">#</a> 1. 性能指标</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm<span class="token punctuation">.</span>engine<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> Metrics

<span class="token comment"># 获取性能指标</span>
metrics <span class="token operator">=</span> Metrics<span class="token punctuation">.</span>get_metrics<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;吞吐量: </span><span class="token interpolation"><span class="token punctuation">{</span>metrics<span class="token punctuation">.</span>throughput<span class="token punctuation">}</span></span><span class="token string"> tokens/s&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;延迟: </span><span class="token interpolation"><span class="token punctuation">{</span>metrics<span class="token punctuation">.</span>latency<span class="token punctuation">}</span></span><span class="token string"> ms&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;GPU利用率: </span><span class="token interpolation"><span class="token punctuation">{</span>metrics<span class="token punctuation">.</span>gpu_utilization<span class="token punctuation">}</span></span><span class="token string">%&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-日志分析" tabindex="-1"><a class="header-anchor" href="#_2-日志分析" aria-hidden="true">#</a> 2. 日志分析</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 启用详细日志</span>
<span class="token keyword">import</span> logging

logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>
    level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">,</span>
    <span class="token builtin">format</span><span class="token operator">=</span><span class="token string">&#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#39;</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ollama监控" tabindex="-1"><a class="header-anchor" href="#ollama监控" aria-hidden="true">#</a> Ollama监控</h3><h4 id="_1-模型状态" tabindex="-1"><a class="header-anchor" href="#_1-模型状态" aria-hidden="true">#</a> 1. 模型状态</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 查看模型状态</span>
ollama <span class="token function">ps</span>

<span class="token comment"># 查看模型信息</span>
ollama show llama3:8b
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-日志查看" tabindex="-1"><a class="header-anchor" href="#_2-日志查看" aria-hidden="true">#</a> 2. 日志查看</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 查看日志</span>
journalctl <span class="token parameter variable">-u</span> ollama <span class="token parameter variable">-f</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="故障排查" tabindex="-1"><a class="header-anchor" href="#故障排查" aria-hidden="true">#</a> 故障排查</h2><h3 id="常见问题" tabindex="-1"><a class="header-anchor" href="#常见问题" aria-hidden="true">#</a> 常见问题</h3><h4 id="_1-vllm问题" tabindex="-1"><a class="header-anchor" href="#_1-vllm问题" aria-hidden="true">#</a> 1. vLLM问题</h4><p><strong>问题：</strong> CUDA Out of Memory</p><p><strong>解决：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 减少并发数</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama3-8B&quot;</span><span class="token punctuation">,</span>
    max_num_seqs<span class="token operator">=</span><span class="token number">64</span>
<span class="token punctuation">)</span>

<span class="token comment"># 降低GPU内存利用率</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    gpu_memory_utilization<span class="token operator">=</span><span class="token number">0.7</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>问题：</strong> 推理速度慢</p><p><strong>解决：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 启用Flash Attention</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    enable_flash_attn<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>

<span class="token comment"># 增加tensor并行</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">&quot;meta-llama/Llama-3-8B&quot;</span><span class="token punctuation">,</span>
    tensor_parallel_size<span class="token operator">=</span><span class="token number">4</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-ollama问题" tabindex="-1"><a class="header-anchor" href="#_2-ollama问题" aria-hidden="true">#</a> 2. Ollama问题</h4><p><strong>问题：</strong> 模型下载失败</p><p><strong>解决：</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 使用镜像源</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">OLLAMA_HOST</span><span class="token operator">=</span>https://ollama.com
ollama pull llama3:8b
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>问题：</strong> 推理速度慢</p><p><strong>解决：</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 使用GPU加速</span>
<span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> ollama run llama3:8b

<span class="token comment"># 使用更小的模型</span>
ollama pull llama3:8b-q4_0
ollama run llama3:8b-q4_0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="未来展望" tabindex="-1"><a class="header-anchor" href="#未来展望" aria-hidden="true">#</a> 未来展望</h2><h3 id="vllm发展方向" tabindex="-1"><a class="header-anchor" href="#vllm发展方向" aria-hidden="true">#</a> vLLM发展方向</h3><ol><li><p><strong>多模态支持</strong></p><ul><li>原生多模态推理</li><li>跨模态注意力机制</li></ul></li><li><p><strong>更高效的架构</strong></p><ul><li>新的注意力机制</li><li>更好的并行策略</li></ul></li><li><p><strong>云原生部署</strong></p><ul><li>Kubernetes集成</li><li>自动扩缩容</li></ul></li></ol><h3 id="ollama发展方向" tabindex="-1"><a class="header-anchor" href="#ollama发展方向" aria-hidden="true">#</a> Ollama发展方向</h3><ol><li><p><strong>轻量化优化</strong></p><ul><li>更小的模型支持</li><li>移动端优化</li></ul></li><li><p><strong>生态扩展</strong></p><ul><li>更多模型格式支持</li><li>更丰富的工具集成</li></ul></li><li><p><strong>企业级功能</strong></p><ul><li>多用户支持</li><li>访问控制</li><li>监控告警</li></ul></li></ol><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h2><p>vLLM和Ollama各有优势，适合不同的使用场景：</p><p><strong>vLLM适合：</strong></p><ul><li>企业级生产环境</li><li>高并发API服务</li><li>多模型并发服务</li><li>分布式部署</li></ul><p><strong>Ollama适合：</strong></p><ul><li>个人开发实验</li><li>快速原型开发</li><li>边缘设备部署</li><li>轻量化场景</li></ul><p><strong>选择建议：</strong></p><ul><li>个人开发：Ollama</li><li>小型团队：Ollama + vLLM混合</li><li>企业服务：vLLM</li></ul><p>根据你的具体需求、硬件资源和应用场景，选择最适合的部署框架，才能充分发挥大模型的价值。</p><hr><p><strong>参考资料：</strong></p><ul><li><a href="https://docs.vllm.ai/" target="_blank" rel="noopener noreferrer">vLLM官方文档<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">Ollama官方文档<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM GitHub<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://github.com/ollama/ollama" target="_blank" rel="noopener noreferrer">Ollama GitHub<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li></ul></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 491750329@qq.com">kevin12369</span><!--]--><!--]--></div></div></footer><!----><!----><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">默认页脚</div><div class="vp-copyright">Copyright © 2026 Kevin</div></footer></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-D3ipPn3n.js" defer></script>
  </body>
</html>
